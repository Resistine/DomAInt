{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Fri Sep 25 14:50:13 2020\n",
    "\n",
    "The model and preprocessing for estimation of domain name suspiciousness\n",
    "\n",
    "We use decoded domains in format <tld>.<domain_name> \n",
    "Preprocessing part split domain name by bigrams, includind dots\n",
    "Here we use bigram vocabulary that formed from benign data set.\n",
    "\n",
    "The model is GRU regression with rmsprop optimizer\n",
    "\n",
    "@author: marina\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "import time\n",
    "import sklearn.metrics as ms\n",
    "#from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/'\n",
    "# Load data (list of domains with labels) from csv file   \n",
    "#data = pd.read_csv('/home/sungria/py_progs/Domains/rawdata/100k_samples-nn1-bs2_uniq.csv')\n",
    "data = pd.read_csv(PATH + 'samples-nn1-bs2_uniq.csv')\n",
    "\t\n",
    "### load bigram vocabulary\n",
    "bigrams_vocab2 = {}\n",
    "with open(PATH + 'bigram_vocabulary_all.json') as json_file:\n",
    "#with open('/home/sungria/py_progs/Domains/bigram_vocabulary_all.json') as json_file:\n",
    "    bigrams_vocab2 = json.load(json_file)\n",
    "    \n",
    "    \n",
    "# X is input data, string with domain\n",
    "X = data['domain']\n",
    "# y is label: 0 - benign, 1 - mailicious\n",
    "y = data['rep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### some useful functions\n",
    "### all domains should be decoded in human-readable format\n",
    "def urlDecode(url1):\n",
    "     try:\n",
    "          r = url1.encode('utf-8')\n",
    "          res = r.decode('idna')\n",
    "     except:\n",
    "          print (\"Can't process domain: \", url1)\n",
    "          res = ''\n",
    "     return res\n",
    "\n",
    "### split domain to bigrams\n",
    "def findBigrams(input_string):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_string : string\n",
    "    Split domain string to bigrams.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bigram_list : list\n",
    "        list of bigrams.\n",
    "\n",
    "    '''\n",
    "    bigram_list = []\n",
    "    for i in range(0, (len(input_string)-1), 1):\n",
    "        bigram_list.append(input_string[i] + input_string[i+1])\n",
    "    return bigram_list\n",
    "\n",
    "### encode bigrams to integers \n",
    "def bigrams2int(bigram_list):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bigram_list : list\n",
    "        Encoding a list of bigrams to the list of integers\n",
    "        If bigram is not in the dictionary, it replaces with 1 (out of vocabulary token).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bigram_int : list\n",
    "        list of integers.\n",
    "\n",
    "    '''\n",
    "    bigram_int = []\n",
    "    for item in bigram_list:\n",
    "         if item in bigrams_vocab2.keys():\n",
    "              bigram_int.append(bigrams_vocab2[item])\n",
    "         else:\n",
    "              bigram_int.append(int(1))               \n",
    "    return bigram_int\n",
    " \n",
    "### Domain preprocessing:\n",
    "### we get data as a domain string, we need to process it to vector format:\n",
    "def preprocessing(domain_str):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    domain_str : string\n",
    "        input is a domain string in format <tld>.<domain_name> :\n",
    "            'com.greycortex'\n",
    "        We need to transform it to the list of bigrams:\n",
    "            co, om, m., .g, gr, re, ey, tc, co, or, rt, te, ex\n",
    "        For embedding layer, we replace each bigram with integer (according to the dictionary).\n",
    "            850, 469,  91 264, 384, 186, 575, 351, 850, 82, 461, 753, 435  \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bigram_int : list\n",
    "        A list of integers where each integer corresponds a bigram.\n",
    "    '''\n",
    "    ### decoding (if necessary)\n",
    "    #domain_str = urlDecode(domain_str)\n",
    "    ### lower case\n",
    "    domain_low = domain_str.lower()\n",
    "    ### replace characters: numbers with 0\n",
    "    domain0 = re.sub('\\d', '0', domain_low)\n",
    "    ### replace non-ascii with ?\n",
    "    domain_ascii = re.sub(r'[^\\.\\-0-9a-z]','?', domain0)\n",
    "    ### create bigrams\n",
    "    bigrams = findBigrams(domain_ascii) # list of bigram\n",
    "    ### encode bigram to integer\n",
    "    int_list = bigrams2int(bigrams) # list of integers\n",
    "                  \n",
    "    return int_list\n",
    "    \n",
    "# Fit models\n",
    "def fit_model(X_train, y_train, model, epochs, batch):\n",
    "   # early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, epochs=epochs ,  \n",
    "                        validation_split=0.2, batch_size=batch, \n",
    "                        shuffle = False) #, callbacks = [early_stop])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing \n",
    "# labels should lay in the interval [0, 1]:\n",
    "y0 = y.map(lambda lb: float((lb + 1)/2))\n",
    "# convert to array \n",
    "y_arr = y0.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain transformation\n",
    "X_int = X.map(lambda x: preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_int.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of each sample\n",
    "len_seq = X_int.map(lambda seq: len(seq))\n",
    "### The sequences have different lengths and Keras prefers inputs to be vectorized \n",
    "### and all inputs to have the same length. So, we use padding:\n",
    "max_length = int(round(len_seq.mean() + 3*len_seq.std()))\n",
    "X_padded = pad_sequences(X_int, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split data to train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_arr, test_size=0.001)\n",
    "### save to file\n",
    "#np.save('X_test_01.npy', X_test)\n",
    "#np.save('y_test_01.npy', y_test)\n",
    "\n",
    "#np.save('X_train_99.npy', X_train)\n",
    "#np.save('y_train_99.npy', y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using mask_zero=True in embedding layer allows flexible input length \n",
    "### Create model\n",
    "### input size =  length of vocabulary + OOV + padding\n",
    "input_size = len(bigrams_vocab2) + 2\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_size, 32, input_length=max_length, mask_zero=True)\n",
    "\n",
    "### model\n",
    "#model_gru64 = tf.keras.Sequential([\n",
    "#    embedding_layer,\n",
    "#    tf.keras.layers.GRU(64, dropout=0.2, go_backwards=True),\n",
    "#    tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "#model_gru64.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "model_lstm64 = tf.keras.Sequential([\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.LSTM(32, dropout=0.2, go_backwards=True),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "model_lstm64.compile(optimizer='rmsprop', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2889016 samples, validate on 722254 samples\n",
      "Epoch 1/25\n",
      "2889016/2889016 [==============================] - 170s 59us/sample - loss: 0.0903 - val_loss: 0.0811\n",
      "Epoch 2/25\n",
      "2889016/2889016 [==============================] - 166s 57us/sample - loss: 0.0800 - val_loss: 0.0761\n",
      "Epoch 3/25\n",
      "2889016/2889016 [==============================] - 166s 58us/sample - loss: 0.0769 - val_loss: 0.0741\n",
      "Epoch 4/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0754 - val_loss: 0.0722\n",
      "Epoch 5/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0744 - val_loss: 0.0717\n",
      "Epoch 6/25\n",
      "2889016/2889016 [==============================] - 166s 58us/sample - loss: 0.0737 - val_loss: 0.0706\n",
      "Epoch 7/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0731 - val_loss: 0.0706\n",
      "Epoch 8/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0727 - val_loss: 0.0698\n",
      "Epoch 9/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0724 - val_loss: 0.0694\n",
      "Epoch 10/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0721 - val_loss: 0.0688\n",
      "Epoch 11/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0719 - val_loss: 0.0692\n",
      "Epoch 12/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0716 - val_loss: 0.0689\n",
      "Epoch 13/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0715 - val_loss: 0.0689\n",
      "Epoch 14/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0713 - val_loss: 0.0684\n",
      "Epoch 15/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0712 - val_loss: 0.0684\n",
      "Epoch 16/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0711 - val_loss: 0.0682\n",
      "Epoch 17/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0710 - val_loss: 0.0684\n",
      "Epoch 18/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0709 - val_loss: 0.0682\n",
      "Epoch 19/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0708 - val_loss: 0.0676\n",
      "Epoch 20/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0707 - val_loss: 0.0678\n",
      "Epoch 21/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0706 - val_loss: 0.0679\n",
      "Epoch 22/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0706 - val_loss: 0.0677\n",
      "Epoch 23/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0705 - val_loss: 0.0678\n",
      "Epoch 24/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0704 - val_loss: 0.0673\n",
      "Epoch 25/25\n",
      "2889016/2889016 [==============================] - 167s 58us/sample - loss: 0.0704 - val_loss: 0.0675\n"
     ]
    }
   ],
   "source": [
    "## training\n",
    "#history_gru64 = fit_model(X_train, y_train, model_gru64, epochs=35, batch=128)\n",
    "history_lstm64 = fit_model(X_train, y_train, model_lstm64, epochs=25, batch=128)\n",
    "#plot_loss (history_gru64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prediction\n",
    "#y_predict_gru64 = model_gru64.predict(X_test)\n",
    "Y_predict_lstm64 = model_lstm64.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/domain_bigrams-furt-2020-11-07T23:13:50/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/domain_bigrams-furt-2020-11-07T23:13:50/assets\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "   \n",
    "saved_model_path = 'models/domain_bigrams-furt-' + format(now)\n",
    "tf.saved_model.save(model_lstm64, saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06270289], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predict_lstm64[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.1-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
